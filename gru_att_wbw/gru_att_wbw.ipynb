{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from recurrent_BatchNorm import recurrent_BatchNorm\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "PAD_TOKEN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RTE(nn.Module):\n",
    "    def __init__(self, input_size, EMBEDDING_DIM, HIDDEN_DIM, WBW_ATTN):\n",
    "        super(RTE, self).__init__()\n",
    "        self.n_embed = EMBEDDING_DIM\n",
    "        self.n_dim = HIDDEN_DIM if HIDDEN_DIM % 2 == 0 else HIDDEN_DIM - 1\n",
    "        self.n_out = 3\n",
    "        self.embedding = nn.Embedding(input_size, self.n_embed).type(dtype)\n",
    "        self.WBW_ATTN = WBW_ATTN\n",
    "    \n",
    "        self.p_gru = nn.GRU(self.n_embed, self.n_dim, bidirectional=False).type(dtype)\n",
    "        self.h_gru = nn.GRU(self.n_embed, self.n_dim, bidirectional=False).type(dtype)\n",
    "        self.out = nn.Linear(self.n_dim, self.n_out).type(dtype)\n",
    "\n",
    "        \n",
    "        # Attention Parameters\n",
    "        self.W_y = nn.Parameter(torch.randn(self.n_dim, self.n_dim).cuda()) if use_cuda else nn.Parameter(torch.randn(self.n_dim, self.n_dim))  # n_dim x n_dim\n",
    "        self.register_parameter('W_y', self.W_y)\n",
    "        self.W_h = nn.Parameter(torch.randn(self.n_dim, self.n_dim).cuda()) if use_cuda else nn.Parameter(torch.randn(self.n_dim, self.n_dim))  # n_dim x n_dim\n",
    "        self.register_parameter('W_h', self.W_h)\n",
    "        self.W_r = nn.Parameter(torch.randn(self.n_dim, self.n_dim).cuda()) if use_cuda else nn.Parameter(torch.randn(self.n_dim, self.n_dim))  # n_dim x n_dim\n",
    "        self.register_parameter('W_r', self.W_r)\n",
    "        self.W_alpha = nn.Parameter(torch.randn(self.n_dim, 1).cuda()) if use_cuda else nn.Parameter(torch.randn(self.n_dim, 1))  # n_dim x 1\n",
    "        self.register_parameter('W_alpha', self.W_alpha)\n",
    "        if WBW_ATTN:\n",
    "            # Since the word by word attention layer is a simple rnn, it suffers from the gradient exploding problem\n",
    "            # A way to circumvent that is having orthonormal initialization of the weight matrix\n",
    "            _W_t = np.random.randn(self.n_dim, self.n_dim)\n",
    "            _W_t_ortho, _ = np.linalg.qr(_W_t)\n",
    "            self.W_t = nn.Parameter(torch.Tensor(_W_t_ortho).cuda()) if use_cuda else nn.Parameter(torch.Tensor(_W_t_ortho))  # n_dim x n_dim\n",
    "            self.register_parameter('W_t', self.W_t)\n",
    "            self.batch_norm_h_r = recurrent_BatchNorm(self.n_dim, 30).type(dtype) # 'MAX_LEN' = 30\n",
    "            self.batch_norm_r_r = recurrent_BatchNorm(self.n_dim, 30).type(dtype)\n",
    "\n",
    "        # Final combination Parameters\n",
    "        self.W_x = nn.Parameter(torch.randn(self.n_dim, self.n_dim).cuda()) if use_cuda else nn.Parameter(torch.randn(self.n_dim, self.n_dim))  # n_dim x n_dim\n",
    "        self.register_parameter('W_x', self.W_x)\n",
    "        self.W_p = nn.Parameter(torch.randn(self.n_dim, self.n_dim).cuda()) if use_cuda else nn.Parameter(torch.randn(self.n_dim, self.n_dim))  # n_dim x n_dim\n",
    "        self.register_parameter('W_p', self.W_p)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden_p = Variable(torch.zeros(1, batch_size, self.n_dim).type(dtype))\n",
    "        return hidden_p\n",
    "\n",
    "    def attn_rnn_init_hidden(self, batch_size):\n",
    "        r_0 = Variable(torch.zeros(batch_size, self.n_dim).type(dtype))\n",
    "        return r_0\n",
    "\n",
    "    def mask_mult(self, o_t, o_tm1, mask_t):\n",
    "        '''\n",
    "            o_t : batch x n\n",
    "            o_tm1 : batch x n\n",
    "            mask_t : batch x 1\n",
    "        '''\n",
    "        # return (mask_t.expand(*o_t.size()) * o_t) + ((1. - mask_t.expand(*o_t.size())) * (o_tm1))\n",
    "        return (o_t * mask_t) + (o_tm1 * (1. - mask_t))\n",
    "\n",
    "    def _gru_forward(self, gru, encoded_s, mask_s, h_0):\n",
    "        '''\n",
    "        inputs :\n",
    "            gru : The GRU unit for which the forward pass is to be computed\n",
    "            encoded_s : T x batch x n_embed\n",
    "            mask_s : T x batch\n",
    "            h_0 : 1 x batch x n_dim\n",
    "        outputs :\n",
    "            o_s : T x batch x n_dim #outut\n",
    "            h_n : 1 x batch x n_dim #hidden\n",
    "        '''\n",
    "        seq_len = encoded_s.size(0)\n",
    "        batch_size = encoded_s.size(1)\n",
    "        o_s = Variable(torch.zeros(seq_len, batch_size, self.n_dim).type(dtype))\n",
    "        h_tm1 = h_0.squeeze(0)  # batch x n_dim\n",
    "        o_tm1 = None\n",
    "\n",
    "        for ix, (x_t, mask_t) in enumerate(zip(encoded_s, mask_s)):\n",
    "            '''\n",
    "                x_t : batch x n_embed\n",
    "                mask_t : batch,\n",
    "            '''\n",
    "            o_t, h_t = gru(x_t.unsqueeze(0), h_tm1.unsqueeze(0))  # o_t : 1 x batch x n_dim\n",
    "                                                                  # h_t : 1 x batch x n_dim\n",
    "            mask_t = mask_t.unsqueeze(1)  # batch x 1\n",
    "            h_t = self.mask_mult(h_t[0], h_tm1, mask_t)  # batch x n_dim\n",
    "\n",
    "            if o_tm1 is not None:\n",
    "                o_t = self.mask_mult(o_t[0], o_tm1, mask_t)\n",
    "            o_tm1 = o_t[0] if o_tm1 is None else o_t\n",
    "            h_tm1 = h_t\n",
    "            o_s[ix] = o_t\n",
    "\n",
    "        return o_s, h_t.unsqueeze(0)\n",
    "\n",
    "    def _attention_forward(self, Y, mask_Y, h, r_tm1=None, index=None):\n",
    "        '''\n",
    "        Computes the Attention Weights over Y using h (and r_tm1 if given)\n",
    "        Returns an attention weighted representation of Y, and the alphas\n",
    "        inputs:\n",
    "            Y : T x batch x n_dim\n",
    "            mask_Y : T x batch\n",
    "            h : batch x n_dim\n",
    "            r_tm1 : batch x n_dim\n",
    "            index : int : The timestep\n",
    "        params:\n",
    "            W_y : n_dim x n_dim\n",
    "            W_h : n_dim x n_dim\n",
    "            W_r : n_dim x n_dim\n",
    "            W_alpha : n_dim x 1\n",
    "        outputs :\n",
    "            r = batch x n_dim\n",
    "            alpha : batch x T\n",
    "        '''\n",
    "        Y = Y.transpose(1, 0)  # batch x T x n_dim\n",
    "        mask_Y = mask_Y.transpose(1, 0)  # batch x T\n",
    "\n",
    "        Wy = torch.bmm(Y, self.W_y.unsqueeze(0).expand(Y.size(0), *self.W_y.size()))  # batch x T x n_dim\n",
    "        Wh = torch.mm(h, self.W_h)  # batch x n_dim\n",
    "        if r_tm1 is not None:\n",
    "            W_r_tm1 = self.batch_norm_r_r(torch.mm(r_tm1, self.W_r), index) if hasattr(self, 'batch_norm_r_r') else torch.mm(r_tm1, self.W_r)\n",
    "            Wh = self.batch_norm_h_r(Wh, index) if hasattr(self, 'batch_norm_h_r') else Wh\n",
    "            Wh += W_r_tm1\n",
    "        M = torch.tanh(Wy + Wh.unsqueeze(1).expand(Wh.size(0), Y.size(1), Wh.size(1)))  # batch x T x n_dim\n",
    "        alpha = torch.bmm(M, self.W_alpha.unsqueeze(0).expand(Y.size(0), *self.W_alpha.size())).squeeze(-1)  # batch x T\n",
    "        alpha = alpha + (-1000.0 * (1. - mask_Y))  # To ensure probability mass doesn't fall on non tokens\n",
    "        alpha = F.softmax(alpha)\n",
    "        if r_tm1 is not None:\n",
    "            r = torch.bmm(alpha.unsqueeze(1), Y).squeeze(1) + F.tanh(torch.mm(r_tm1, self.W_t))  # batch x n_dim\n",
    "        else:\n",
    "            r = torch.bmm(alpha.unsqueeze(1), Y).squeeze(1)  # batch x n_dim\n",
    "        return r, alpha\n",
    "\n",
    "    def _combine_last(self, r, h_t):\n",
    "        '''\n",
    "        inputs:\n",
    "            r : batch x n_dim\n",
    "            h_t : batch x n_dim (this is the output from the gru unit)\n",
    "        params :\n",
    "            W_x : n_dim x n_dim\n",
    "            W_p : n_dim x n_dim\n",
    "        out :\n",
    "            h_star : batch x n_dim\n",
    "        '''\n",
    "\n",
    "        W_p_r = torch.mm(r, self.W_p)  # batch x n_dim\n",
    "        W_x_h = torch.mm(h_t, self.W_x)  # batch x n_dim\n",
    "        h_star = F.tanh(W_p_r + W_x_h)  # batch x n_dim\n",
    "\n",
    "        return h_star\n",
    "\n",
    "    def _attn_rnn_forward(self, o_h, mask_h, r_0, o_p, mask_p):\n",
    "        '''\n",
    "        inputs:\n",
    "            o_h : T x batch x n_dim : The hypothesis\n",
    "            mask_h : T x batch\n",
    "            r_0 : batch x n_dim\n",
    "            o_p : T x batch x n_dim : The premise. Will attend on it at every step\n",
    "            mask_p : T x batch : the mask for the premise\n",
    "        params:\n",
    "            W_t : n_dim x n_dim\n",
    "        outputs:\n",
    "            r : batch x n_dim : the last state of the rnn\n",
    "            alpha_vec : T x batch x T the attn vec at every step\n",
    "        '''\n",
    "        seq_len_h = o_h.size(0)\n",
    "        batch_size = o_h.size(1)\n",
    "        seq_len_p = o_p.size(0)\n",
    "        alpha_vec = Variable(torch.zeros(seq_len_h, batch_size, seq_len_p).type(dtype))\n",
    "        r_tm1 = r_0\n",
    "        for ix, (h_t, mask_t) in enumerate(zip(o_h, mask_h)):\n",
    "            '''\n",
    "                h_t : batch x n_dim\n",
    "                mask_t : batch,\n",
    "            '''\n",
    "            r_t, alpha = self._attention_forward(o_p, mask_p, h_t, r_tm1, ix)   # r_t : batch x n_dim\n",
    "                                                                                # alpha : batch x T\n",
    "            alpha_vec[ix] = alpha\n",
    "            mask_t = mask_t.unsqueeze(1)  # batch x 1\n",
    "            r_t = self.mask_mult(r_t, r_tm1, mask_t)\n",
    "            r_tm1 = r_t\n",
    "\n",
    "        return r_t, alpha_vec\n",
    "\n",
    "    def forward(self, premise, hypothesis, training=False):\n",
    "        '''\n",
    "        inputs:\n",
    "            premise : batch x T\n",
    "            hypothesis : batch x T\n",
    "        outputs :\n",
    "            pred : batch x num_classes\n",
    "        '''\n",
    "        self.train(training)\n",
    "        batch_size = premise.size(0)\n",
    "\n",
    "        mask_p = torch.ne(premise, 0).type(dtype)\n",
    "        mask_h = torch.ne(hypothesis, 0).type(dtype)\n",
    "\n",
    "        encoded_p = self.embedding(premise)  # batch x T x n_embed\n",
    "        encoded_p = F.dropout(encoded_p, p=0.1, training=training)\n",
    "\n",
    "        encoded_h = self.embedding(hypothesis)  # batch x T x n_embed\n",
    "        encoded_h = F.dropout(encoded_h, p=0.1, training=training)\n",
    "\n",
    "        encoded_p = encoded_p.transpose(1, 0)  # T x batch x n_embed\n",
    "        encoded_h = encoded_h.transpose(1, 0)  # T x batch x n_embed\n",
    "\n",
    "        mask_p = mask_p.transpose(1, 0)  # T x batch\n",
    "        mask_h = mask_h.transpose(1, 0)  # T x batch\n",
    "\n",
    "        h_0 = self.init_hidden(batch_size)  # 1 x batch x n_dim\n",
    "        o_p, h_n = self._gru_forward(self.p_gru, encoded_p, mask_p, h_0)  # o_p : T x batch x n_dim\n",
    "                                                                          # h_n : 1 x batch x n_dim\n",
    "\n",
    "        o_h, h_n = self._gru_forward(self.h_gru, encoded_h, mask_h, h_n)  # o_h : T x batch x n_dim\n",
    "                                                                          # h_n : 1 x batch x n_dim\n",
    "\n",
    "        if self.WBW_ATTN:\n",
    "            r_0 = self.attn_rnn_init_hidden(batch_size)  # batch x n_dim\n",
    "            r, alpha_vec = self._attn_rnn_forward(o_h, mask_h, r_0, o_p, mask_p)  # r : batch x n_dim\n",
    "                                                                                  # alpha_vec : T x batch x T         \n",
    "        else:\n",
    "            r, alpha = self._attention_forward(o_p, mask_p, o_h[-1])  # r : batch x n_dim\n",
    "                                                                      # alpha : batch x T\n",
    "\n",
    "        h_star = self._combine_last(r, o_h[-1])  # batch x n_dim\n",
    "        h_star = self.out(h_star)  # batch x num_classes\n",
    "        '''\n",
    "        if self.options['LAST_NON_LINEAR']:\n",
    "            h_star = F.leaky_relu(h_star)  # Non linear projection\n",
    "        '''\n",
    "        pred = F.log_softmax(h_star)\n",
    "        \n",
    "        return pred\n",
    "\n",
    "    def _get_numpy_array_from_variable(self, variable):\n",
    "        '''\n",
    "        Converts a torch autograd variable to its corresponding numpy array\n",
    "        '''\n",
    "        if use_cuda:\n",
    "            return variable.cpu().data.numpy()\n",
    "        else:\n",
    "            return variable.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading snli_1.0.zip\n",
      "extracting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [09:35, 1.50MB/s]                               \n",
      "100%|██████████| 400000/400000 [01:24<00:00, 4727.75it/s]\n"
     ]
    }
   ],
   "source": [
    "inputs = datasets.snli.ParsedTextField(lower=True)\n",
    "answers = data.Field(sequential=False)\n",
    "\n",
    "train, dev, test = datasets.SNLI.splits(inputs, answers)\n",
    "\n",
    "# get input embeddings\n",
    "inputs.build_vocab(train, vectors='glove.6B.300d')\n",
    "answers.build_vocab(train)\n",
    "\n",
    "# global params\n",
    "global input_size, num_train_steps\n",
    "vocab_size = len(inputs.vocab)\n",
    "input_size = vocab_size\n",
    "num_train_steps = 50000000\n",
    "\n",
    "train_iter, dev_iter, test_iter = data.BucketIterator.splits((train, dev, test), batch_size=32, device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RTE(input_size, EMBEDDING_DIM = 100, HIDDEN_DIM = 300, WBW_ATTN = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_loop(model, loss, optimizer, train_iter, dev_iter):\n",
    "    step = 0\n",
    "    for i in range(num_train_steps):\n",
    "        model.train()\n",
    "        for batch in train_iter:\n",
    "            premise = batch.premise.transpose(0, 1)\n",
    "            hypothesis = batch.hypothesis.transpose(0, 1)\n",
    "            labels = batch.label - 1\n",
    "            model.zero_grad()\n",
    "        \n",
    "            output = model(premise, hypothesis)\n",
    "            \n",
    "            lossy = loss(output, labels)\n",
    "            #print(lossy)\n",
    "            lossy.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print( \"Step %i; Loss %f; Dev acc %f\" %(step, lossy.data[0], evaluate(model, dev_iter)))\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_iter):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in data_iter:\n",
    "        premise = batch.premise.transpose(0,1)\n",
    "        hypothesis = batch.hypothesis.transpose(0,1)\n",
    "        labels = (batch.label-1).data\n",
    "        output = model(premise, hypothesis)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    model.train()\n",
    "    return correct / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sean/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:5: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0; Loss 1.088470; Dev acc 0.336212\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b4a5e7a5c586>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mbest_dev_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_dev_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-46feca53725e>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, loss, optimizer, train_iter, dev_iter)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"Step %i; Loss %f; Dev acc %f\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-85663b97dbcb>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, data_iter)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mhypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpremise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0b1376b39f65>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, premise, hypothesis, training)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mh_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 1 x batch x n_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mo_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gru_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_gru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# o_p : T x batch x n_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m                                                                           \u001b[0;31m# h_n : 1 x batch x n_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0b1376b39f65>\u001b[0m in \u001b[0;36m_gru_forward\u001b[0;34m(self, gru, encoded_s, mask_s, h_0)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mo_tm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             '''\n\u001b[1;32m     76\u001b[0m                 \u001b[0mx_t\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0mx\u001b[0m \u001b[0mn_embed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__mod__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mIndexSelect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;31m# else fall through and raise an error in Index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, index)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loss\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "para2 = model.parameters()\n",
    "optimizer = torch.optim.Adagrad(para2, lr=0.001, weight_decay=5e-5)\n",
    "\n",
    "# Train the model\n",
    "best_dev_acc = training_loop(model, loss, optimizer, train_iter, dev_iter)\n",
    "print(best_dev_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
