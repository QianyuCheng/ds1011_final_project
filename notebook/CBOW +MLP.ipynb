{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = datasets.snli.ParsedTextField(lower=True)\n",
    "answers = data.Field(sequential=False)\n",
    "\n",
    "train, dev, test = datasets.SNLI.splits(inputs, answers)\n",
    "\n",
    "inputs.build_vocab(train, dev, test)\n",
    "answers.build_vocab(train)\n",
    "\n",
    "train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n",
    "            (train, dev, test), batch_size=64, device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64280"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They', 'are', 'smiling', 'at', 'their', 'parents']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[3].hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 10 \n",
      "    17     56      3   4901      3      3      3   1677     52   6155     13\n",
      "   177    102      7   2545   2047    543      7    459     30    504    762\n",
      "    82      8      5   4706    245     11    230     64     40  15800    174\n",
      "    90     24     70   3383      5     88     55     10      4   5995    253\n",
      "    10  22000      8    229    155      8      2    176      2     61    690\n",
      "    42      4      2      6      2      2    648     15    267      6      9\n",
      "   458     86   5462  10027   3337    118   1057    976  18280   1590  10197\n",
      "\n",
      "Columns 11 to 21 \n",
      "     3     17     49      3    145      3      3     54     17      2      2\n",
      "    27     18     10   2359     18     22    511    145     14    130    592\n",
      "    31     10  14042    171     39      7     18      7    176  15220    998\n",
      "    39     39    163      5     15      4    135    100      2     55      2\n",
      "   308     61     77     21    240      2     16      2   1389     81  17071\n",
      "     2     79    411      6     61    569      6    258      2    657   2518\n",
      " 14863     85   1938   1656   1353    486   1100   5426    635   1500    778\n",
      "\n",
      "Columns 22 to 32 \n",
      "    17     49      3    143      3      3     13     17      3     13    694\n",
      "    18     10      7     18    678     25     31    102   2032     88    102\n",
      "  3233  14042      4     10    176      4      5    279     14    382    105\n",
      "    68    163      2   2539     40   4320     70    120      5      4    124\n",
      "    15     77     83      4      6      8    237     75    106      6      8\n",
      "     2    411      5      2    413   2389      6      8      2     58      6\n",
      "   231   1938   3528    156    498    321   1766   3201    327    151    151\n",
      "\n",
      "Columns 33 to 43 \n",
      "     3      3     52    116  15446      3      3      3      3     17   2272\n",
      "    60    165    115      9     37     37    369     43     37     29      9\n",
      "    26   6415     84     14     11     11      5     62     11     88    270\n",
      "     4      5      2    404     18     18     65   2522     18     10    395\n",
      "     2  13721   4082      4      4    774      2      2     10     70     59\n",
      "    66      2     11      6      6     61    530    600    114      8  17351\n",
      "   288    542   7394    306   1011   1371   1530    153  37099   2074  31841\n",
      "\n",
      "Columns 44 to 54 \n",
      "   694      3     17     52      3    285  31920      3     17      2      3\n",
      "    25  18848     28      8     31     19  13378     31     46     22    440\n",
      "    12    107     21      2     93    793     10    204      8     14    827\n",
      "  2277   1031    420    302      4      5   1015    193    993     93   3862\n",
      "  1448     16      8     16    865    122     73      6   1673     15    305\n",
      "  4953      2      6      2    426      6     79    375    120      2      2\n",
      "   768   1349   1371   1370    179   2831  10259    151    289   2576    712\n",
      "\n",
      "Columns 55 to 63 \n",
      "     3   2414     13      3      3      3      3     54     13\n",
      "    26     18    227     27    270     26     47    254     14\n",
      "     5     96      4     31     34     90      4      7      5\n",
      "   748      8      6     39      8      5      2     42     21\n",
      "    55      2    168      4      2    454    322      2     12\n",
      "  1887    258      5      6    178      2     19  15868      2\n",
      "  1281   1105   7377    209    536   2497  39896   2440    872\n",
      "[torch.LongTensor of size 7x64]\n",
      "\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 10 \n",
      "    17    561     13   2545      2     49     13    393     52      6     13\n",
      "   690     10      7   4706   2047     10      7    630     39    946    762\n",
      "    82     21      5   3383    245  18749      5     10     61   1214     10\n",
      "    10      4     16    229    449      8     24   1800      2      4   4839\n",
      " 20049      6      6      6      2      6   6991     15    267      6      9\n",
      " 20854     85   1014  10027   4087    157   9994   1065    365   1487  29652\n",
      "\n",
      "Columns 11 to 21 \n",
      "     3     56     13      3     52      3     52     54     13      3      3\n",
      "    27     18   2397     27     11     22     10    145     14      7     43\n",
      "    31     10      5      7    145      7    135      7     10    489      5\n",
      "     5     39    317      5   3010      4     36    566     21      8      4\n",
      "  3354     61      4     21     10      2      6     20    701      2      6\n",
      " 48670     86  17112   1656    486   1100   1100    879   1281   1500    306\n",
      "\n",
      "Columns 22 to 32 \n",
      "    17  27156     13    143      3      3     13     17      3  22667     13\n",
      "    82     12      7     18    646     25     31    138     14     88     64\n",
      "    10  14042      5    392      5      5   2386     10      5     10     10\n",
      "   146     57    725     16    119      4    237     70     42    454    167\n",
      "     2     22      2      6  24589      6      6      2      2  22667      2\n",
      " 21178     47   6891    156    207    187   1766    348    211   4491    153\n",
      "\n",
      "Columns 33 to 43 \n",
      "     3     13    149  17931      3      3      3     13     13     17     49\n",
      "    47    165     18    997    101     37    369     43   1747     29     10\n",
      "     5      5   2402    170     11     11      5     62    132     88     18\n",
      "    19    500    141     48     18     18    106      2     89    454    162\n",
      "     2     16    893    695     12   1671      2    600   2325      2  29029\n",
      "   288    460  11694   7535   4165   1637   2230    153  29889   1290  22011\n",
      "\n",
      "Columns 44 to 54 \n",
      "     3     13     13     13      3      3     52     13      3     13      3\n",
      "    60    107     28     18     31     43     10     31  28166     14    440\n",
      "    25      5     10     10     93      5   2521      5    112    175    827\n",
      "     5      6     21      8     36      4      2    454     61     15      5\n",
      "  4953  14423      2     24      2      6    987      2      2      2      4\n",
      "   768   3028   1578   1688   1833    509   2413   6328   1511   2576    532\n",
      "\n",
      "Columns 55 to 63 \n",
      "    49     49     13      3      3      3     13      3      3\n",
      "     5     10    168   1000    270     26     47      7     14\n",
      "     2     18     62      5    261      5      5     62    100\n",
      "    43     96    227     59      8    454     65      2     12\n",
      "    12      8      4      4      2      2      2    562     35\n",
      "  1281    136    219   1700    118   2497   4379   1916    872\n",
      "[torch.LongTensor of size 6x64]\n",
      "\n",
      "Variable containing:\n",
      " 3\n",
      " 1\n",
      " 3\n",
      " 1\n",
      " 1\n",
      " 3\n",
      " 3\n",
      " 3\n",
      " 2\n",
      " 3\n",
      " 2\n",
      " 3\n",
      " 2\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 1\n",
      " 1\n",
      " 3\n",
      " 3\n",
      " 3\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 3\n",
      " 1\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 3\n",
      " 3\n",
      " 2\n",
      " 2\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 2\n",
      " 1\n",
      " 3\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 2\n",
      " 3\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 3\n",
      " 3\n",
      " 2\n",
      " 1\n",
      " 2\n",
      " 1\n",
      " 3\n",
      "[torch.LongTensor of size 64]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(batch.premise)\n",
    "print(batch.hypothesis)\n",
    "print(batch.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A Multi-Layer Perceptron (MLP)\n",
    "class MLPClassifier(nn.Module): # inheriting from nn.Module!\n",
    "    \n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, num_labels):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        # Define the parameters that you will need.  \n",
    "        # You need an embedding matrix, parameters for affine mappings and ReLus\n",
    "        # Pay attention to dimensions!\n",
    "        \n",
    "        self.embed = nn.Embedding(input_size, embedding_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "            \n",
    "        self.linear_1 = nn.Linear(2*embedding_dim, hidden_dim) \n",
    "        self.linear_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear_3 = nn.Linear(hidden_dim, num_labels)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, prem, hypo):\n",
    "        # Pass the input through your layers in order\n",
    "        emb_prem = self.embed(prem).mean(1)\n",
    "        emb_hypo = self.embed(hypo).mean(1)\n",
    "        emb_concat = torch.cat([emb_prem, emb_hypo],1)\n",
    "        out = self.dropout(emb_concat)\n",
    "        out = F.relu(self.linear_1(out))\n",
    "        out = F.relu(self.linear_2(out))\n",
    "        out = self.dropout(self.linear_3(out))\n",
    "        return F.log_softmax(out)\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        lin_layers = [self.linear_1, self.linear_2]\n",
    "        em_layer = [self.embed]\n",
    "     \n",
    "        for layer in lin_layers+em_layer:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            if layer in lin_layers:\n",
    "                layer.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_loop(model, loss, optimizer, train_iter, dev_iter):\n",
    "    step = 0\n",
    "    for i in range(num_train_steps):\n",
    "        model.train()\n",
    "        for batch in train_iter:\n",
    "            premise = batch.premise.transpose(0,1)\n",
    "            hypothesis = batch.hypothesis.transpose(0,1)\n",
    "            labels = batch.label-1\n",
    "            model.zero_grad()\n",
    "            output = model(premise, hypothesis)\n",
    "            lossy = loss(output, labels)\n",
    "            #print(lossy)\n",
    "            lossy.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print( \"Step %i; Loss %f; Dev acc %f\" \n",
    "                %(step, lossy.data[0], evaluate(model, dev_iter)))\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_iter):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in data_iter:\n",
    "        premise = batch.premise.transpose(0,1)\n",
    "        hypothesis = batch.hypothesis.transpose(0,1)\n",
    "        labels = (batch.label-1).data\n",
    "        output = model(premise, hypothesis)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    model.train()\n",
    "    return correct / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(inputs.vocab)\n",
    "input_size = vocab_size\n",
    "num_labels = 3\n",
    "hidden_dim = 50\n",
    "embedding_dim = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.004\n",
    "num_train_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:5: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0; Loss 1.075891; Dev acc 0.328693\n",
      "Step 10; Loss 1.115394; Dev acc 0.329709\n",
      "Step 20; Loss 1.100434; Dev acc 0.334078\n",
      "Step 30; Loss 1.096971; Dev acc 0.356127\n",
      "Step 40; Loss 1.083404; Dev acc 0.354908\n",
      "Step 50; Loss 1.081633; Dev acc 0.405609\n",
      "Step 60; Loss 1.076163; Dev acc 0.401951\n",
      "Step 70; Loss 1.141551; Dev acc 0.405304\n",
      "Step 80; Loss 1.102221; Dev acc 0.486893\n",
      "Step 90; Loss 1.043828; Dev acc 0.495529\n",
      "Step 100; Loss 1.078658; Dev acc 0.488823\n",
      "Step 110; Loss 1.090243; Dev acc 0.506198\n",
      "Step 120; Loss 1.094956; Dev acc 0.525198\n",
      "Step 130; Loss 0.953534; Dev acc 0.521540\n",
      "Step 140; Loss 1.060090; Dev acc 0.517273\n",
      "Step 150; Loss 1.090013; Dev acc 0.525097\n",
      "Step 160; Loss 1.037443; Dev acc 0.525503\n",
      "Step 170; Loss 1.068275; Dev acc 0.539016\n",
      "Step 180; Loss 1.002581; Dev acc 0.528856\n",
      "Step 190; Loss 1.052827; Dev acc 0.541963\n",
      "Step 200; Loss 1.168307; Dev acc 0.554765\n",
      "Step 210; Loss 0.993622; Dev acc 0.554867\n",
      "Step 220; Loss 0.936447; Dev acc 0.545824\n",
      "Step 230; Loss 0.991443; Dev acc 0.557712\n",
      "Step 240; Loss 0.984696; Dev acc 0.546637\n",
      "Step 250; Loss 1.015809; Dev acc 0.542369\n",
      "Step 260; Loss 0.963106; Dev acc 0.555375\n",
      "Step 270; Loss 1.117995; Dev acc 0.540744\n",
      "Step 280; Loss 1.039173; Dev acc 0.563605\n",
      "Step 290; Loss 1.024593; Dev acc 0.574172\n",
      "Step 300; Loss 0.922978; Dev acc 0.572953\n",
      "Step 310; Loss 1.090690; Dev acc 0.571937\n",
      "Step 320; Loss 0.966194; Dev acc 0.578135\n",
      "Step 330; Loss 0.996203; Dev acc 0.578846\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(input_size, embedding_dim, hidden_dim, num_labels)\n",
    "    \n",
    "# Loss and Optimizer\n",
    "loss = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "training_loop(model, loss, optimizer, train_iter, dev_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
