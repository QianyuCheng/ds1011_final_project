{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = datasets.snli.ParsedTextField(lower=True)\n",
    "answers = data.Field(sequential=False)\n",
    "\n",
    "train, dev, test = datasets.SNLI.splits(inputs, answers)\n",
    "\n",
    "inputs.build_vocab(train, dev, test)\n",
    "answers.build_vocab(train)\n",
    "\n",
    "train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n",
    "            (train, dev, test), batch_size=64, device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64280"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They', 'are', 'smiling', 'at', 'their', 'parents']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[3].hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 10 \n",
      "    17     56      3   4901      3      3      3   1677     52   6155     13\n",
      "   177    102      7   2545   2047    543      7    459     30    504    762\n",
      "    82      8      5   4706    245     11    230     64     40  15800    174\n",
      "    90     24     70   3383      5     88     55     10      4   5995    253\n",
      "    10  22000      8    229    155      8      2    176      2     61    690\n",
      "    42      4      2      6      2      2    648     15    267      6      9\n",
      "   458     86   5462  10027   3337    118   1057    976  18280   1590  10197\n",
      "\n",
      "Columns 11 to 21 \n",
      "     3     17     49      3    145      3      3     54     17      2      2\n",
      "    27     18     10   2359     18     22    511    145     14    130    592\n",
      "    31     10  14042    171     39      7     18      7    176  15220    998\n",
      "    39     39    163      5     15      4    135    100      2     55      2\n",
      "   308     61     77     21    240      2     16      2   1389     81  17071\n",
      "     2     79    411      6     61    569      6    258      2    657   2518\n",
      " 14863     85   1938   1656   1353    486   1100   5426    635   1500    778\n",
      "\n",
      "Columns 22 to 32 \n",
      "    17     49      3    143      3      3     13     17      3     13    694\n",
      "    18     10      7     18    678     25     31    102   2032     88    102\n",
      "  3233  14042      4     10    176      4      5    279     14    382    105\n",
      "    68    163      2   2539     40   4320     70    120      5      4    124\n",
      "    15     77     83      4      6      8    237     75    106      6      8\n",
      "     2    411      5      2    413   2389      6      8      2     58      6\n",
      "   231   1938   3528    156    498    321   1766   3201    327    151    151\n",
      "\n",
      "Columns 33 to 43 \n",
      "     3      3     52    116  15446      3      3      3      3     17   2272\n",
      "    60    165    115      9     37     37    369     43     37     29      9\n",
      "    26   6415     84     14     11     11      5     62     11     88    270\n",
      "     4      5      2    404     18     18     65   2522     18     10    395\n",
      "     2  13721   4082      4      4    774      2      2     10     70     59\n",
      "    66      2     11      6      6     61    530    600    114      8  17351\n",
      "   288    542   7394    306   1011   1371   1530    153  37099   2074  31841\n",
      "\n",
      "Columns 44 to 54 \n",
      "   694      3     17     52      3    285  31920      3     17      2      3\n",
      "    25  18848     28      8     31     19  13378     31     46     22    440\n",
      "    12    107     21      2     93    793     10    204      8     14    827\n",
      "  2277   1031    420    302      4      5   1015    193    993     93   3862\n",
      "  1448     16      8     16    865    122     73      6   1673     15    305\n",
      "  4953      2      6      2    426      6     79    375    120      2      2\n",
      "   768   1349   1371   1370    179   2831  10259    151    289   2576    712\n",
      "\n",
      "Columns 55 to 63 \n",
      "     3   2414     13      3      3      3      3     54     13\n",
      "    26     18    227     27    270     26     47    254     14\n",
      "     5     96      4     31     34     90      4      7      5\n",
      "   748      8      6     39      8      5      2     42     21\n",
      "    55      2    168      4      2    454    322      2     12\n",
      "  1887    258      5      6    178      2     19  15868      2\n",
      "  1281   1105   7377    209    536   2497  39896   2440    872\n",
      "[torch.LongTensor of size 7x64]\n",
      "\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 10 \n",
      "    17    561     13   2545      2     49     13    393     52      6     13\n",
      "   690     10      7   4706   2047     10      7    630     39    946    762\n",
      "    82     21      5   3383    245  18749      5     10     61   1214     10\n",
      "    10      4     16    229    449      8     24   1800      2      4   4839\n",
      " 20049      6      6      6      2      6   6991     15    267      6      9\n",
      " 20854     85   1014  10027   4087    157   9994   1065    365   1487  29652\n",
      "\n",
      "Columns 11 to 21 \n",
      "     3     56     13      3     52      3     52     54     13      3      3\n",
      "    27     18   2397     27     11     22     10    145     14      7     43\n",
      "    31     10      5      7    145      7    135      7     10    489      5\n",
      "     5     39    317      5   3010      4     36    566     21      8      4\n",
      "  3354     61      4     21     10      2      6     20    701      2      6\n",
      " 48670     86  17112   1656    486   1100   1100    879   1281   1500    306\n",
      "\n",
      "Columns 22 to 32 \n",
      "    17  27156     13    143      3      3     13     17      3  22667     13\n",
      "    82     12      7     18    646     25     31    138     14     88     64\n",
      "    10  14042      5    392      5      5   2386     10      5     10     10\n",
      "   146     57    725     16    119      4    237     70     42    454    167\n",
      "     2     22      2      6  24589      6      6      2      2  22667      2\n",
      " 21178     47   6891    156    207    187   1766    348    211   4491    153\n",
      "\n",
      "Columns 33 to 43 \n",
      "     3     13    149  17931      3      3      3     13     13     17     49\n",
      "    47    165     18    997    101     37    369     43   1747     29     10\n",
      "     5      5   2402    170     11     11      5     62    132     88     18\n",
      "    19    500    141     48     18     18    106      2     89    454    162\n",
      "     2     16    893    695     12   1671      2    600   2325      2  29029\n",
      "   288    460  11694   7535   4165   1637   2230    153  29889   1290  22011\n",
      "\n",
      "Columns 44 to 54 \n",
      "     3     13     13     13      3      3     52     13      3     13      3\n",
      "    60    107     28     18     31     43     10     31  28166     14    440\n",
      "    25      5     10     10     93      5   2521      5    112    175    827\n",
      "     5      6     21      8     36      4      2    454     61     15      5\n",
      "  4953  14423      2     24      2      6    987      2      2      2      4\n",
      "   768   3028   1578   1688   1833    509   2413   6328   1511   2576    532\n",
      "\n",
      "Columns 55 to 63 \n",
      "    49     49     13      3      3      3     13      3      3\n",
      "     5     10    168   1000    270     26     47      7     14\n",
      "     2     18     62      5    261      5      5     62    100\n",
      "    43     96    227     59      8    454     65      2     12\n",
      "    12      8      4      4      2      2      2    562     35\n",
      "  1281    136    219   1700    118   2497   4379   1916    872\n",
      "[torch.LongTensor of size 6x64]\n",
      "\n",
      "Variable containing:\n",
      " 3\n",
      " 1\n",
      " 3\n",
      " 1\n",
      " 1\n",
      " 3\n",
      " 3\n",
      " 3\n",
      " 2\n",
      " 3\n",
      " 2\n",
      " 3\n",
      " 2\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 1\n",
      " 1\n",
      " 3\n",
      " 3\n",
      " 3\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 3\n",
      " 1\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 3\n",
      " 3\n",
      " 2\n",
      " 2\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 2\n",
      " 1\n",
      " 3\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 2\n",
      " 3\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 3\n",
      " 3\n",
      " 2\n",
      " 1\n",
      " 2\n",
      " 1\n",
      " 3\n",
      "[torch.LongTensor of size 64]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(batch.premise)\n",
    "print(batch.hypothesis)\n",
    "print(batch.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A Multi-Layer Perceptron (MLP)\n",
    "class MLPClassifier(nn.Module): # inheriting from nn.Module!\n",
    "    \n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, num_labels):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        # Define the parameters that you will need.  \n",
    "        # You need an embedding matrix, parameters for affine mappings and ReLus\n",
    "        # Pay attention to dimensions!\n",
    "        \n",
    "        self.embed = nn.Embedding(input_size, embedding_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "            \n",
    "        self.linear_1 = nn.Linear(2*embedding_dim, hidden_dim) \n",
    "        self.linear_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear_3 = nn.Linear(hidden_dim, num_labels)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, prem, hypo):\n",
    "        # Pass the input through your layers in order\n",
    "        emb_prem = self.embed(prem).mean(1)\n",
    "        emb_hypo = self.embed(hypo).mean(1)\n",
    "        emb_concat = torch.cat([emb_prem, emb_hypo],1)\n",
    "        out = self.dropout(emb_concat)\n",
    "        out = F.relu(self.linear_1(out))\n",
    "        out = F.relu(self.linear_2(out))\n",
    "        out = self.dropout(self.linear_3(out))\n",
    "        return F.log_softmax(out)\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        lin_layers = [self.linear_1, self.linear_2]\n",
    "        em_layer = [self.embed]\n",
    "     \n",
    "        for layer in lin_layers+em_layer:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            if layer in lin_layers:\n",
    "                layer.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_loop(model, loss, optimizer, train_iter, dev_iter):\n",
    "    step = 0\n",
    "    for i in range(num_train_steps):\n",
    "        model.train()\n",
    "        for batch in train_iter:\n",
    "            premise = batch.premise.transpose(0,1)\n",
    "            hypothesis = batch.hypothesis.transpose(0,1)\n",
    "            labels = batch.label-1\n",
    "            model.zero_grad()\n",
    "            output = model(premise, hypothesis)\n",
    "            lossy = loss(output, labels)\n",
    "            #print(lossy)\n",
    "            lossy.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print( \"Step %i; Loss %f; Dev acc %f\" \n",
    "                %(step, lossy.data[0], evaluate(model, dev_iter)))\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_iter):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in data_iter:\n",
    "        premise = batch.premise.transpose(0,1)\n",
    "        hypothesis = batch.hypothesis.transpose(0,1)\n",
    "        labels = (batch.label-1).data\n",
    "        output = model(premise, hypothesis)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    return correct / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(inputs.vocab)\n",
    "input_size = vocab_size\n",
    "num_labels = 3\n",
    "hidden_dim = 50\n",
    "embedding_dim = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.004\n",
    "num_train_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:5: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0; Loss 1.092246; Dev acc 0.338244\n",
      "Step 10; Loss 1.095578; Dev acc 0.336720\n",
      "Step 20; Loss 1.100651; Dev acc 0.333062\n",
      "Step 30; Loss 1.102983; Dev acc 0.333062\n",
      "Step 40; Loss 1.099342; Dev acc 0.417192\n",
      "Step 50; Loss 1.084354; Dev acc 0.431721\n",
      "Step 60; Loss 0.989143; Dev acc 0.442390\n",
      "Step 70; Loss 1.022728; Dev acc 0.489941\n",
      "Step 80; Loss 1.030023; Dev acc 0.525198\n",
      "Step 90; Loss 1.037210; Dev acc 0.522150\n",
      "Step 100; Loss 1.014148; Dev acc 0.527129\n",
      "Step 110; Loss 0.787402; Dev acc 0.539728\n",
      "Step 120; Loss 0.908419; Dev acc 0.553444\n",
      "Step 130; Loss 0.925783; Dev acc 0.555273\n",
      "Step 140; Loss 1.031158; Dev acc 0.562182\n",
      "Step 150; Loss 1.003083; Dev acc 0.554257\n",
      "Step 160; Loss 0.944038; Dev acc 0.571124\n",
      "Step 170; Loss 0.829129; Dev acc 0.566856\n",
      "Step 180; Loss 1.008304; Dev acc 0.571937\n",
      "Step 190; Loss 0.907513; Dev acc 0.574375\n",
      "Step 200; Loss 0.813880; Dev acc 0.566145\n",
      "Step 210; Loss 1.010208; Dev acc 0.577525\n",
      "Step 220; Loss 0.995342; Dev acc 0.570514\n",
      "Step 230; Loss 0.905400; Dev acc 0.578744\n",
      "Step 240; Loss 0.696807; Dev acc 0.571937\n",
      "Step 250; Loss 0.858170; Dev acc 0.579557\n",
      "Step 260; Loss 0.875877; Dev acc 0.584028\n",
      "Step 270; Loss 0.865891; Dev acc 0.584942\n",
      "Step 280; Loss 0.835427; Dev acc 0.590226\n",
      "Step 290; Loss 0.746594; Dev acc 0.594696\n",
      "Step 300; Loss 0.758043; Dev acc 0.592969\n",
      "Step 310; Loss 0.958761; Dev acc 0.592562\n",
      "Step 320; Loss 0.976102; Dev acc 0.589006\n",
      "Step 330; Loss 0.810656; Dev acc 0.588092\n",
      "Step 340; Loss 0.780191; Dev acc 0.583824\n",
      "Step 350; Loss 0.870217; Dev acc 0.592461\n",
      "Step 360; Loss 0.923224; Dev acc 0.591750\n",
      "Step 370; Loss 0.971172; Dev acc 0.590226\n",
      "Step 380; Loss 0.944178; Dev acc 0.591140\n",
      "Step 390; Loss 0.908378; Dev acc 0.592562\n",
      "Step 400; Loss 0.928654; Dev acc 0.595103\n",
      "Step 410; Loss 0.958814; Dev acc 0.587279\n",
      "Step 420; Loss 1.027909; Dev acc 0.597744\n",
      "Step 430; Loss 0.894644; Dev acc 0.602113\n",
      "Step 440; Loss 0.885114; Dev acc 0.591648\n",
      "Step 450; Loss 0.941114; Dev acc 0.599776\n",
      "Step 460; Loss 0.868163; Dev acc 0.603231\n",
      "Step 470; Loss 0.881972; Dev acc 0.600284\n",
      "Step 480; Loss 0.977503; Dev acc 0.598862\n",
      "Step 490; Loss 0.756815; Dev acc 0.595915\n",
      "Step 500; Loss 0.834756; Dev acc 0.601301\n",
      "Step 510; Loss 0.974418; Dev acc 0.602520\n",
      "Step 520; Loss 0.814191; Dev acc 0.604450\n",
      "Step 530; Loss 0.829519; Dev acc 0.606076\n",
      "Step 540; Loss 0.808147; Dev acc 0.605974\n",
      "Step 550; Loss 0.895590; Dev acc 0.601910\n",
      "Step 560; Loss 0.877430; Dev acc 0.606381\n",
      "Step 570; Loss 0.866266; Dev acc 0.608819\n",
      "Step 580; Loss 0.794313; Dev acc 0.607194\n",
      "Step 590; Loss 0.763920; Dev acc 0.602418\n",
      "Step 600; Loss 0.827999; Dev acc 0.604145\n",
      "Step 610; Loss 0.784818; Dev acc 0.602825\n",
      "Step 620; Loss 0.888310; Dev acc 0.605771\n",
      "Step 630; Loss 0.817077; Dev acc 0.603536\n",
      "Step 640; Loss 0.947331; Dev acc 0.603129\n",
      "Step 650; Loss 0.988266; Dev acc 0.608007\n",
      "Step 660; Loss 0.829071; Dev acc 0.609429\n",
      "Step 670; Loss 0.843702; Dev acc 0.589819\n",
      "Step 680; Loss 0.810270; Dev acc 0.599776\n",
      "Step 690; Loss 0.643751; Dev acc 0.602317\n",
      "Step 700; Loss 0.835038; Dev acc 0.606178\n",
      "Step 710; Loss 0.872327; Dev acc 0.608819\n",
      "Step 720; Loss 0.943772; Dev acc 0.606990\n",
      "Step 730; Loss 0.836326; Dev acc 0.608413\n",
      "Step 740; Loss 0.835080; Dev acc 0.614814\n",
      "Step 750; Loss 0.893486; Dev acc 0.609429\n",
      "Step 760; Loss 0.911227; Dev acc 0.606178\n",
      "Step 770; Loss 0.935706; Dev acc 0.610343\n",
      "Step 780; Loss 0.825964; Dev acc 0.601605\n",
      "Step 790; Loss 0.743246; Dev acc 0.612985\n",
      "Step 800; Loss 0.860803; Dev acc 0.616948\n",
      "Step 810; Loss 0.906577; Dev acc 0.619183\n",
      "Step 820; Loss 0.883347; Dev acc 0.615729\n",
      "Step 830; Loss 0.667835; Dev acc 0.616745\n",
      "Step 840; Loss 0.974749; Dev acc 0.612782\n",
      "Step 850; Loss 0.822471; Dev acc 0.615830\n",
      "Step 860; Loss 0.910791; Dev acc 0.611359\n",
      "Step 870; Loss 0.750936; Dev acc 0.613493\n",
      "Step 880; Loss 0.905023; Dev acc 0.612579\n",
      "Step 890; Loss 0.853626; Dev acc 0.616541\n",
      "Step 900; Loss 0.861759; Dev acc 0.616846\n",
      "Step 910; Loss 0.726666; Dev acc 0.610445\n",
      "Step 920; Loss 0.749480; Dev acc 0.615932\n",
      "Step 930; Loss 0.908692; Dev acc 0.621926\n",
      "Step 940; Loss 0.899120; Dev acc 0.623857\n",
      "Step 950; Loss 0.832574; Dev acc 0.622739\n",
      "Step 960; Loss 0.898992; Dev acc 0.621012\n",
      "Step 970; Loss 0.801047; Dev acc 0.620301\n",
      "Step 980; Loss 0.910509; Dev acc 0.620809\n",
      "Step 990; Loss 0.771213; Dev acc 0.620504\n",
      "Step 1000; Loss 0.814074; Dev acc 0.623654\n",
      "Step 1010; Loss 0.960008; Dev acc 0.622739\n",
      "Step 1020; Loss 0.797058; Dev acc 0.620606\n",
      "Step 1030; Loss 0.794564; Dev acc 0.618980\n",
      "Step 1040; Loss 0.791793; Dev acc 0.625991\n",
      "Step 1050; Loss 0.969177; Dev acc 0.623349\n",
      "Step 1060; Loss 0.881607; Dev acc 0.623146\n",
      "Step 1070; Loss 0.949215; Dev acc 0.626092\n",
      "Step 1080; Loss 0.791134; Dev acc 0.621622\n",
      "Step 1090; Loss 0.854286; Dev acc 0.622333\n",
      "Step 1100; Loss 0.877212; Dev acc 0.626194\n",
      "Step 1110; Loss 0.828405; Dev acc 0.625076\n",
      "Step 1120; Loss 0.775067; Dev acc 0.625991\n",
      "Step 1130; Loss 0.874199; Dev acc 0.624975\n",
      "Step 1140; Loss 0.850942; Dev acc 0.628124\n",
      "Step 1150; Loss 0.859195; Dev acc 0.626397\n",
      "Step 1160; Loss 0.744483; Dev acc 0.627413\n",
      "Step 1170; Loss 0.828464; Dev acc 0.625381\n",
      "Step 1180; Loss 0.856563; Dev acc 0.625686\n",
      "Step 1190; Loss 0.808960; Dev acc 0.625686\n",
      "Step 1200; Loss 0.858656; Dev acc 0.626194\n",
      "Step 1210; Loss 0.894928; Dev acc 0.629445\n",
      "Step 1220; Loss 0.912136; Dev acc 0.625483\n",
      "Step 1230; Loss 0.835486; Dev acc 0.627312\n",
      "Step 1240; Loss 0.809453; Dev acc 0.625686\n",
      "Step 1250; Loss 0.774385; Dev acc 0.625889\n",
      "Step 1260; Loss 0.725906; Dev acc 0.622739\n",
      "Step 1270; Loss 0.821826; Dev acc 0.625076\n",
      "Step 1280; Loss 0.698730; Dev acc 0.617862\n",
      "Step 1290; Loss 0.851155; Dev acc 0.628632\n",
      "Step 1300; Loss 0.715487; Dev acc 0.627108\n",
      "Step 1310; Loss 0.775600; Dev acc 0.628734\n",
      "Step 1320; Loss 0.795580; Dev acc 0.627921\n",
      "Step 1330; Loss 0.933207; Dev acc 0.628124\n",
      "Step 1340; Loss 0.957206; Dev acc 0.635338\n",
      "Step 1350; Loss 0.878930; Dev acc 0.629242\n",
      "Step 1360; Loss 0.968973; Dev acc 0.631782\n",
      "Step 1370; Loss 0.995863; Dev acc 0.626702\n",
      "Step 1380; Loss 0.824438; Dev acc 0.629140\n",
      "Step 1390; Loss 0.758097; Dev acc 0.626397\n",
      "Step 1400; Loss 0.853662; Dev acc 0.630969\n",
      "Step 1410; Loss 0.971413; Dev acc 0.632697\n",
      "Step 1420; Loss 0.764680; Dev acc 0.634221\n",
      "Step 1430; Loss 0.944693; Dev acc 0.635338\n",
      "Step 1440; Loss 1.020637; Dev acc 0.633916\n",
      "Step 1450; Loss 0.709347; Dev acc 0.630461\n",
      "Step 1460; Loss 0.885635; Dev acc 0.634221\n",
      "Step 1470; Loss 0.785707; Dev acc 0.636862\n",
      "Step 1480; Loss 0.692345; Dev acc 0.637269\n",
      "Step 1490; Loss 1.043207; Dev acc 0.633103\n",
      "Step 1500; Loss 0.830098; Dev acc 0.635440\n",
      "Step 1510; Loss 0.777442; Dev acc 0.634830\n",
      "Step 1520; Loss 0.804142; Dev acc 0.634526\n",
      "Step 1530; Loss 0.723648; Dev acc 0.629852\n",
      "Step 1540; Loss 0.813156; Dev acc 0.630868\n",
      "Step 1550; Loss 0.729385; Dev acc 0.635643\n",
      "Step 1560; Loss 0.769722; Dev acc 0.632798\n",
      "Step 1570; Loss 1.003722; Dev acc 0.637269\n",
      "Step 1580; Loss 0.783633; Dev acc 0.637269\n",
      "Step 1590; Loss 0.779872; Dev acc 0.635237\n",
      "Step 1600; Loss 0.862814; Dev acc 0.633814\n",
      "Step 1610; Loss 0.666035; Dev acc 0.633103\n",
      "Step 1620; Loss 0.727187; Dev acc 0.639707\n",
      "Step 1630; Loss 0.862356; Dev acc 0.641841\n",
      "Step 1640; Loss 0.925324; Dev acc 0.640114\n",
      "Step 1650; Loss 0.854813; Dev acc 0.639199\n",
      "Step 1660; Loss 0.921945; Dev acc 0.636659\n",
      "Step 1670; Loss 0.826972; Dev acc 0.631274\n",
      "Step 1680; Loss 0.822917; Dev acc 0.629242\n",
      "Step 1690; Loss 0.897841; Dev acc 0.638082\n",
      "Step 1700; Loss 0.705171; Dev acc 0.640114\n",
      "Step 1710; Loss 0.707111; Dev acc 0.636253\n",
      "Step 1720; Loss 0.916831; Dev acc 0.640622\n",
      "Step 1730; Loss 0.884141; Dev acc 0.639403\n",
      "Step 1740; Loss 0.751131; Dev acc 0.643264\n",
      "Step 1750; Loss 0.788439; Dev acc 0.643670\n",
      "Step 1760; Loss 0.965146; Dev acc 0.640520\n",
      "Step 1770; Loss 0.857394; Dev acc 0.637574\n",
      "Step 1780; Loss 0.864549; Dev acc 0.640723\n",
      "Step 1790; Loss 0.814489; Dev acc 0.641231\n",
      "Step 1800; Loss 0.751742; Dev acc 0.641028\n",
      "Step 1810; Loss 0.832047; Dev acc 0.645092\n",
      "Step 1820; Loss 0.722891; Dev acc 0.646820\n",
      "Step 1830; Loss 0.966935; Dev acc 0.636354\n",
      "Step 1840; Loss 0.722774; Dev acc 0.645092\n",
      "Step 1850; Loss 0.911943; Dev acc 0.644788\n",
      "Step 1860; Loss 0.871194; Dev acc 0.641333\n",
      "Step 1870; Loss 0.752719; Dev acc 0.641536\n",
      "Step 1880; Loss 0.694097; Dev acc 0.644076\n",
      "Step 1890; Loss 0.759735; Dev acc 0.642552\n",
      "Step 1900; Loss 0.707853; Dev acc 0.642146\n",
      "Step 1910; Loss 0.710006; Dev acc 0.650579\n",
      "Step 1920; Loss 0.961039; Dev acc 0.650274\n",
      "Step 1930; Loss 0.806957; Dev acc 0.643162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1940; Loss 0.843748; Dev acc 0.640825\n",
      "Step 1950; Loss 0.861337; Dev acc 0.642959\n",
      "Step 1960; Loss 0.763894; Dev acc 0.641231\n",
      "Step 1970; Loss 0.632962; Dev acc 0.640723\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(input_size, embedding_dim, hidden_dim, num_labels)\n",
    "    \n",
    "# Loss and Optimizer\n",
    "loss = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "training_loop(model, loss, optimizer, train_iter, dev_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
