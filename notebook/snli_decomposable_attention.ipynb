{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT = datasets.snli.ParsedTextField(lower=True)\n",
    "\n",
    "LABEL = data.Field(sequential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = datasets.SNLI.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': <torchtext.datasets.snli.ParsedTextField object at 0x1150bfba8>, 'hypothesis': <torchtext.datasets.snli.ParsedTextField object at 0x1150bfba8>, 'label': <torchtext.data.field.Field object at 0x11508b940>}\n",
      "549367\n",
      "{'premise': ['a', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane.'], 'hypothesis': ['a', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition.'], 'label': 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train, val, test), batch_size=5, device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109874"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 5])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.premise.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "     1      1      1      1      1\n",
      "  1101      1      1      1      1\n",
      "    59      1      1      1      1\n",
      "   116      1      1      1      1\n",
      "    10    444   2633   3343    445\n",
      "    40     19     47   1646    417\n",
      "     4      4    329     71     15\n",
      "   416     95  10717    251   1057\n",
      "  1420      2     10     60    373\n",
      "   891     11    887    897      3\n",
      "    84     82      2   2215     15\n",
      "    14      3     11      2    277\n",
      "    20      4     28     17     15\n",
      "    10   2194      5      5    157\n",
      "    33     29     12     12     76\n",
      "     2      2      2      3     13\n",
      "[torch.LongTensor of size 16x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(batch.premise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1\n",
      " 1\n",
      " 3\n",
      " 3\n",
      " 3\n",
      "[torch.LongTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(batch.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "import argparse\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings, embedding_size, hidden_size, para_init):\n",
    "        super(encoder, self).__init__()\n",
    "\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.para_init = para_init\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_size)\n",
    "        self.input_linear = nn.Linear(self.embedding_size, self.hidden_size, bias=False)\n",
    "        \n",
    "        # input_linear initialize a normal weught for each input\n",
    "        for m in self.modules():\n",
    "            # check if module is torch.nn.modules.linear.Linear, which is used to select input_linear\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, self.para_init)\n",
    "                # m.bias.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "    def forward(self, sent1, sent2):\n",
    "        '''\n",
    "        sent: batch_size x length (Long tensor)\n",
    "        '''\n",
    "        batch_size = sent1.size(0)\n",
    "        sent1 = self.embedding(sent1)\n",
    "        sent2 = self.embedding(sent2)\n",
    "\n",
    "        sent1 = sent1.view(-1, self.embedding_size)\n",
    "        sent2 = sent2.view(-1, self.embedding_size)\n",
    "\n",
    "        sent1_linear = self.input_linear(sent1).view(batch_size, -1, self.hidden_size)\n",
    "        sent2_linear = self.input_linear(sent2).view(batch_size, -1, self.hidden_size)\n",
    "\n",
    "        return sent1_linear, sent2_linear\n",
    "\n",
    "class atten(nn.Module):\n",
    "    '''\n",
    "        intra sentence attention\n",
    "    '''\n",
    "\n",
    "    def __init__(self, hidden_size, label_size, para_init):\n",
    "        super(atten, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.label_size = label_size\n",
    "        self.para_init = para_init\n",
    "\n",
    "        self.mlp_f = self._mlp_layers(self.hidden_size, self.hidden_size)\n",
    "        self.mlp_g = self._mlp_layers(2 * self.hidden_size, self.hidden_size)\n",
    "        self.mlp_h = self._mlp_layers(2 * self.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.final_linear = nn.Linear(\n",
    "            self.hidden_size, self.label_size, bias=True)\n",
    "\n",
    "        self.log_prob = nn.LogSoftmax()\n",
    "\n",
    "        '''initialize parameters'''\n",
    "        for m in self.modules():\n",
    "            # print m\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, self.para_init)\n",
    "                m.bias.data.normal_(0, self.para_init)\n",
    "\n",
    "    def _mlp_layers(self, input_dim, output_dim):\n",
    "        mlp_layers = []\n",
    "        mlp_layers.append(nn.Dropout(p=0.2))\n",
    "        mlp_layers.append(nn.Linear(\n",
    "            input_dim, output_dim, bias=True))\n",
    "        mlp_layers.append(nn.ReLU())\n",
    "        mlp_layers.append(nn.Dropout(p=0.2))\n",
    "        mlp_layers.append(nn.Linear(\n",
    "            output_dim, output_dim, bias=True))\n",
    "        mlp_layers.append(nn.ReLU())        \n",
    "        return nn.Sequential(*mlp_layers)   # * used to unpack list\n",
    "\n",
    "    def forward(self, sent1_linear, sent2_linear):\n",
    "        '''\n",
    "            sent_linear: batch_size x length x hidden_size\n",
    "        '''\n",
    "        len1 = sent1_linear.size(1)\n",
    "        len2 = sent2_linear.size(1)\n",
    "\n",
    "        '''attend'''\n",
    "\n",
    "        f1 = self.mlp_f(sent1_linear.view(-1, self.hidden_size))\n",
    "        f2 = self.mlp_f(sent2_linear.view(-1, self.hidden_size))\n",
    "\n",
    "        f1 = f1.view(-1, len1, self.hidden_size)\n",
    "        # batch_size x len1 x hidden_size\n",
    "        f2 = f2.view(-1, len2, self.hidden_size)\n",
    "        # batch_size x len2 x hidden_size\n",
    "\n",
    "        score1 = torch.bmm(f1, torch.transpose(f2, 1, 2))\n",
    "        # e_{ij} batch_size x len1 x len2\n",
    "        prob1 = F.softmax(score1.view(-1, len2)).view(-1, len1, len2)\n",
    "        # batch_size x len1 x len2\n",
    "\n",
    "        score2 = torch.transpose(score1.contiguous(), 1, 2)\n",
    "        score2 = score2.contiguous()\n",
    "        # e_{ji} batch_size x len2 x len1\n",
    "        prob2 = F.softmax(score2.view(-1, len1)).view(-1, len2, len1)\n",
    "        # batch_size x len2 x len1\n",
    "\n",
    "        sent1_combine = torch.cat(\n",
    "            (sent1_linear, torch.bmm(prob1, sent2_linear)), 2)\n",
    "        # batch_size x len1 x (hidden_size x 2)\n",
    "        sent2_combine = torch.cat(\n",
    "            (sent2_linear, torch.bmm(prob2, sent1_linear)), 2)\n",
    "        # batch_size x len2 x (hidden_size x 2)\n",
    "\n",
    "        '''sum'''\n",
    "        g1 = self.mlp_g(sent1_combine.view(-1, 2 * self.hidden_size))\n",
    "        g2 = self.mlp_g(sent2_combine.view(-1, 2 * self.hidden_size))\n",
    "        g1 = g1.view(-1, len1, self.hidden_size)\n",
    "        # batch_size x len1 x hidden_size\n",
    "        g2 = g2.view(-1, len2, self.hidden_size)\n",
    "        # batch_size x len2 x hidden_size\n",
    "\n",
    "        sent1_output = torch.sum(g1, 1)  # batch_size x 1 x hidden_size\n",
    "        sent1_output = torch.squeeze(sent1_output, 1)\n",
    "        sent2_output = torch.sum(g2, 1)  # batch_size x 1 x hidden_size\n",
    "        sent2_output = torch.squeeze(sent2_output, 1)\n",
    "\n",
    "        input_combine = torch.cat((sent1_output, sent2_output), 1)\n",
    "        # batch_size x (2 * hidden_size)\n",
    "        h = self.mlp_h(input_combine)\n",
    "        # batch_size * hidden_size\n",
    "\n",
    "        # if sample_id == 15:\n",
    "        #     print '-2 layer'\n",
    "        #     print h.data[:, 100:150]\n",
    "\n",
    "        h = self.final_linear(h)\n",
    "\n",
    "        # print 'final layer'\n",
    "        # print h.data\n",
    "\n",
    "        log_prob = self.log_prob(h)\n",
    "\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lenght = 10\n",
    "embedding_size = 50\n",
    "hidden_size = 300\n",
    "para_init = 0.01  \n",
    "train_lbl_size = 3\n",
    "weight_decay = 5e-5 #l2 regularization\n",
    "learning_rate = 0.05\n",
    "epoch = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pretrained word vectors from glove\n",
    "\n",
    "glove_home = './glove_6B/'\n",
    "words_to_load = 50000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# pre-trained word vectors\n",
    "with open(glove_home + 'glove.6B.50d.txt') as f:\n",
    "    word_vecs = np.zeros((words_to_load, embedding_size)) #dim: (50000, 50)\n",
    "    words = {}\n",
    "    idx2words = {}\n",
    "    ordered_words = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        \n",
    "        word_vecs[i, :] = np.asarray(s[1:])\n",
    "        words[s[0]] = i\n",
    "        idx2words[i] = s[0]\n",
    "        ordered_words.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = torch.from_numpy(word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 50])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vecs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encoder = encoder(word_vecs.size(0), embedding_size, hidden_size, para_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encoder.embedding.weight.data.copy_(word_vecs)\n",
    "input_encoder.embedding.weight.requires_grad = False\n",
    "inter_atten = atten(hidden_size, train_lbl_size, para_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para1 = filter(lambda p: p.requires_grad, input_encoder.parameters())\n",
    "para2 = inter_atten.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_optimizer = optim.Adagrad(para1, learning_rate, weight_decay)\n",
    "inter_atten_optimizer = optim.Adagrad(para2, learning_rate, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches  = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Batch' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-270-9656a2c72ae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtimer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/random.py\u001b[0m in \u001b[0;36mshuffle\u001b[0;34m(self, x, random)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mrandbelow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m                 \u001b[0;31m# pick an element in x[:i+1] with which to exchange x[i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'Batch' has no len()"
     ]
    }
   ],
   "source": [
    "#train\n",
    "for k in range(epoch):\n",
    "    total = 0.\n",
    "    correct = 0.\n",
    "    loss_data = 0.\n",
    "    train_sents = 0.\n",
    "\n",
    "    shuffle(train_batches)\n",
    "    timer = time.time()\n",
    "    for i in range(len(train_batches)):\n",
    "\n",
    "        train_src_batch, train_tgt_batch, train_lbl_batch = train_batches[i]\n",
    "\n",
    "        train_src_batch = Variable(train_src_batch)\n",
    "        train_tgt_batch = Variable(train_tgt_batch)\n",
    "        train_lbl_batch = Variable(train_lbl_batch)\n",
    "\n",
    "        batch_size = train_src_batch.size(0)\n",
    "        train_sents += batch_size\n",
    "\n",
    "        input_optimizer.zero_grad()\n",
    "        inter_atten_optimizer.zero_grad()\n",
    "\n",
    "        # initialize the optimizer\n",
    "        if k == 0 and optim == 'Adagrad':\n",
    "            for group in input_optimizer.param_groups:\n",
    "                for p in group['params']:\n",
    "                    state = input_optimizer.state[p]\n",
    "                    state['sum'] += args.Adagrad_init\n",
    "            for group in inter_atten_optimizer.param_groups:\n",
    "                for p in group['params']:\n",
    "                    state = inter_atten_optimizer.state[p]\n",
    "                    state['sum'] += args.Adagrad_init\n",
    "\n",
    "        train_src_linear, train_tgt_linear = input_encoder(\n",
    "            train_src_batch, train_tgt_batch)\n",
    "        log_prob = inter_atten(train_src_linear, train_tgt_linear)\n",
    "\n",
    "        loss = criterion(log_prob, train_lbl_batch)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        grad_norm = 0.\n",
    "        para_norm = 0.\n",
    "\n",
    "        for m in input_encoder.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                grad_norm += m.weight.grad.data.norm() ** 2\n",
    "                para_norm += m.weight.data.norm() ** 2\n",
    "                if m.bias:\n",
    "                    grad_norm += m.bias.grad.data.norm() ** 2\n",
    "                    para_norm += m.bias.data.norm() ** 2\n",
    "\n",
    "        for m in inter_atten.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                grad_norm += m.weight.grad.data.norm() ** 2\n",
    "                para_norm += m.weight.data.norm() ** 2\n",
    "                if m.bias:\n",
    "                    grad_norm += m.bias.grad.data.norm() ** 2\n",
    "                    para_norm += m.bias.data.norm() ** 2\n",
    "\n",
    "        grad_norm ** 0.5\n",
    "        para_norm ** 0.5\n",
    "\n",
    "        shrinkage = args.max_grad_norm / grad_norm\n",
    "        if shrinkage < 1 :\n",
    "            for m in input_encoder.modules():\n",
    "                # print m\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    m.weight.grad.data = m.weight.grad.data * shrinkage\n",
    "            for m in inter_atten.modules():\n",
    "                # print m\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    m.weight.grad.data = m.weight.grad.data * shrinkage\n",
    "                    m.bias.grad.data = m.bias.grad.data * shrinkage\n",
    "\n",
    "        input_optimizer.step()\n",
    "        inter_atten_optimizer.step()\n",
    "\n",
    "        _, predict = log_prob.data.max(dim=1)\n",
    "        total += train_lbl_batch.data.size()[0]\n",
    "        correct += torch.sum(predict == train_lbl_batch.data)\n",
    "        loss_data += (loss.data[0] * batch_size)  # / train_lbl_batch.data.size()[0])\n",
    "\n",
    "        if (i + 1) % args.display_interval == 0:\n",
    "            logger.info('epoch %d, batches %d|%d, train-acc %.3f, loss %.3f, para-norm %.3f, grad-norm %.3f, time %.2fs, ' %\n",
    "                        (k, i + 1, len(train_batches), correct / total,\n",
    "                         loss_data / train_sents, para_norm, grad_norm, time.time() - timer))\n",
    "            train_sents = 0.\n",
    "            timer = time.time()\n",
    "            loss_data = 0.\n",
    "            correct = 0.\n",
    "            total = 0.\n",
    "        if i == len(train_batches) - 1:\n",
    "            logger.info('epoch %d, batches %d|%d, train-acc %.3f, loss %.3f, para-norm %.3f, grad-norm %.3f, time %.2fs, ' %\n",
    "                        (k, i + 1, len(train_batches), correct / total,\n",
    "                         loss_data / train_sents, para_norm, grad_norm, time.time() - timer))\n",
    "            train_sents = 0.\n",
    "            timer = time.time()\n",
    "            loss_data = 0.\n",
    "            correct = 0.\n",
    "            total = 0.           \n",
    "\n",
    "    # evaluate\n",
    "    if (k + 1) % args.dev_interval == 0:\n",
    "        input_encoder.eval()\n",
    "        inter_atten.eval()\n",
    "        correct = 0.\n",
    "        total = 0.\n",
    "        for i in range(len(dev_batches)):\n",
    "            dev_src_batch, dev_tgt_batch, dev_lbl_batch = dev_batches[i]\n",
    "\n",
    "            dev_src_batch = Variable(dev_src_batch.cuda())\n",
    "            dev_tgt_batch = Variable(dev_tgt_batch.cuda())\n",
    "            dev_lbl_batch = Variable(dev_lbl_batch.cuda())\n",
    "\n",
    "            # if dev_lbl_batch.data.size(0) == 1:\n",
    "            #     # simple sample batch\n",
    "            #     dev_src_batch=torch.unsqueeze(dev_src_batch, 0)\n",
    "            #     dev_tgt_batch=torch.unsqueeze(dev_tgt_batch, 0)\n",
    "\n",
    "            dev_src_linear, dev_tgt_linear=input_encoder(\n",
    "                dev_src_batch, dev_tgt_batch)\n",
    "            log_prob=inter_atten(dev_src_linear, dev_tgt_linear)\n",
    "\n",
    "            _, predict=log_prob.data.max(dim=1)\n",
    "            total += dev_lbl_batch.data.size()[0]\n",
    "            correct += torch.sum(predict == dev_lbl_batch.data)\n",
    "\n",
    "        dev_acc = correct / total\n",
    "        logger.info('dev-acc %.3f' % (dev_acc))\n",
    "\n",
    "        if (k + 1) / args.dev_interval == 1:\n",
    "            model_fname = '%s%s_epoch-%d_dev-acc-%.3f' %(args.model_path, args.log_fname.split('.')[0], k, dev_acc)\n",
    "            torch.save(input_encoder.state_dict(), model_fname + '_input-encoder.pt')\n",
    "            torch.save(inter_atten.state_dict(), model_fname + '_inter-atten.pt')\n",
    "            best_dev.append((k, dev_acc, model_fname))\n",
    "            logger.info('current best-dev:')\n",
    "            for t in best_dev:\n",
    "                logger.info('\\t%d %.3f' %(t[0], t[1]))\n",
    "            logger.info('save model!') \n",
    "        else:\n",
    "            if dev_acc > best_dev[-1][1]:\n",
    "                model_fname = '%s%s_epoch-%d_dev-acc-%.3f' %(args.model_path, args.log_fname.split('.')[0], k, dev_acc)\n",
    "                torch.save(input_encoder.state_dict(), model_fname + '_input-encoder.pt')\n",
    "                torch.save(inter_atten.state_dict(), model_fname + '_inter-atten.pt')\n",
    "                best_dev.append((k, dev_acc, model_fname))\n",
    "                logger.info('current best-dev:')\n",
    "                for t in best_dev:\n",
    "                    logger.info('\\t%d %.3f' %(t[0], t[1]))\n",
    "                logger.info('save model!') \n",
    "\n",
    "        input_encoder.train()\n",
    "        inter_atten.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "best_model_fname = best_dev[-1][2]\n",
    "input_encoder.load_state_dict(torch.load(best_model_fname + '_input-encoder.pt'))\n",
    "inter_atten.load_state_dict(torch.load(best_model_fname + '_inter-atten.pt'))\n",
    "\n",
    "input_encoder.eval()\n",
    "inter_atten.eval()\n",
    "\n",
    "correct = 0.\n",
    "total = 0.\n",
    "\n",
    "for i in range(len(test_batches)):\n",
    "    test_src_batch, test_tgt_batch, test_lbl_batch = test_batches[i]\n",
    "\n",
    "    test_src_batch = Variable(test_src_batch.cuda())\n",
    "    test_tgt_batch = Variable(test_tgt_batch.cuda())\n",
    "    test_lbl_batch = Variable(test_lbl_batch.cuda())\n",
    "\n",
    "    test_src_linear, test_tgt_linear=input_encoder(\n",
    "        test_src_batch, test_tgt_batch)\n",
    "    log_prob=inter_atten(test_src_linear, test_tgt_linear)\n",
    "\n",
    "    _, predict=log_prob.data.max(dim=1)\n",
    "    total += test_lbl_batch.data.size()[0]\n",
    "    correct += torch.sum(predict == test_lbl_batch.data)\n",
    "\n",
    "test_acc = correct / total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
